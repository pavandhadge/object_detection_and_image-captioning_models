{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XV8UAGiff8f7",
        "outputId": "d36b8595-1ee6-471f-cb1e-0f4d8e06eaad"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision transformers pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVAM0ljYkSRc",
        "outputId": "87c76f18-bc0c-43c9-99a3-94681d3607ed"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pXaXsk9wmTo-",
        "outputId": "8ebf779f-5601-4c77-e318-1f5dfc8104c1"
      },
      "outputs": [],
      "source": [
        "#  yolo\n",
        "\"\"\"\n",
        "Object Detection with Text-Caption Matching using YOLOv8 and spaCy\n",
        "===================================================================\n",
        "\n",
        "\n",
        "Goal:\n",
        "-----\n",
        "This script combines Natural Language Processing (NLP) and Computer Vision (CV) to:\n",
        "1. Understand the contents of an image.\n",
        "2. Understand the contents of a caption (text description).\n",
        "3. Match objects mentioned in the caption with objects actually detected in the image.\n",
        "4. Highlight those matched objects in the image visually.\n",
        "\n",
        "High-Level Workflow:\n",
        "--------------------\n",
        "1. **Text Object Extraction**:\n",
        "   - The caption (e.g., \"A person sitting on a bed with a laptop\") is processed using spaCy.\n",
        "   - We extract **nouns** from the caption, assuming nouns represent physical objects (like \"person\", \"bed\", \"laptop\").\n",
        "\n",
        "2. **Image Object Detection**:\n",
        "   - The image is processed using **YOLOv8**, a state-of-the-art object detection model.\n",
        "   - It detects all objects in the image and gives us:\n",
        "     - Object class names (e.g., \"person\", \"bottle\")\n",
        "     - Bounding box coordinates\n",
        "     - Confidence scores\n",
        "\n",
        "3. **Object Matching**:\n",
        "   - We compare the list of **nouns from the caption** with the list of **detected objects from YOLO**.\n",
        "   - If there's a match (e.g., \"person\" is in both lists), we consider it a valid detection relevant to the caption.\n",
        "\n",
        "4. **Visualization**:\n",
        "   - For each matched object, we draw a **bounding box** on the image.\n",
        "   - The box is labeled with the object name and detection confidence (e.g., \"person (0.92)\").\n",
        "\n",
        "Use Cases:\n",
        "----------\n",
        "- Visual Grounding: Verifying if objects described in a caption actually exist in an image.\n",
        "- Caption-Aware Filtering: Show only those detections that are relevant to a given description.\n",
        "- Dataset Validation: Ensure that caption annotations are consistent with image content.\n",
        "\n",
        "Dependencies:\n",
        "-------------\n",
        "- `cv2` (OpenCV): For image reading and drawing boxes.\n",
        "- `spacy`: For natural language parsing and POS tagging.\n",
        "- `torch`, `ultralytics`: For loading and running YOLOv8 model.\n",
        "- `matplotlib.pyplot`: To display the final image with overlaid detections.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import spacy\n",
        "from ultralytics import YOLO  # For object detection\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def extract_objects(text):\n",
        "    doc = nlp(text)\n",
        "    objects = []\n",
        "    for token in doc:\n",
        "        if token.pos_ in [\"NOUN\"]:\n",
        "            objects.append(token.text)\n",
        "    print(objects)\n",
        "    return objects\n",
        "\n",
        "# caption = \"A child in a pink dress is climbing up a set of stairs in an entryway.\"\n",
        "# print(extract_objects(caption))\n",
        "model = YOLO(\"yolov8x.pt\")  # Use a larger model\n",
        "\n",
        "\n",
        "def detect_objects(image_path):\n",
        "    results = model(image_path, conf=0.3)\n",
        "    print(results)\n",
        "    return results\n",
        "\n",
        "def match_objects(results, text_objects):\n",
        "    matched = []\n",
        "    for result in results:\n",
        "        for box in result.boxes:\n",
        "            cls = result.names[int(box.cls.item())]  # Object name\n",
        "            prob = box.conf.item()  # Probability\n",
        "            if cls in text_objects:  # If detected object matches caption objects\n",
        "                matched.append((cls, box.xyxy.tolist()[0], prob))\n",
        "    print(matched)\n",
        "    return matched\n",
        "\n",
        "\n",
        "def draw_boxes(image_path, matches):\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    for obj_name, bbox, prob in matches:\n",
        "        x1, y1, x2, y2 = map(int, bbox)\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green Box\n",
        "        cv2.putText(image, f\"{obj_name} ({prob:.2f})\", (x1, y1 - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "image_path = \"/content/20250326_232148.jpg\"\n",
        "caption = \"person sitting on bed with a laptop and window at background with few bottles placed there\"\n",
        "\n",
        "text_objects = extract_objects(caption)\n",
        "print(text_objects)\n",
        "results = detect_objects(image_path)\n",
        "# print(results)\n",
        "matched_objects = match_objects(results, text_objects)\n",
        "draw_boxes(image_path, matched_objects)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Plu8WB2ghzoC",
        "outputId": "6170b0e5-1def-4915-8768-e49758fe31d5"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # This is also required for wordnet to work properly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "LmsLfV7uf_O7",
        "outputId": "74491d55-98c3-4a4f-92e7-1916cdec13c6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Object Detection and Matching using YOLOv8 and CLIP\n",
        "\n",
        "## Overview:\n",
        "This script detects objects in an image using **YOLOv8 (You Only Look Once)** and matches them with text descriptions using **CLIP (Contrastive Language-Image Pretraining)**. It improves object-text matching by expanding textual descriptions with **WordNet synonyms** and **spaCy NLP processing**.\n",
        "\n",
        "## Steps:\n",
        "1. **Load Dependencies & Models**\n",
        "   - YOLOv8 for object detection (`yolov8x.pt`)\n",
        "   - CLIP for computing text-image similarity\n",
        "   - spaCy for text parsing and noun extraction\n",
        "   - WordNet (NLTK) to expand object names with synonyms\n",
        "\n",
        "2. **Download and Prepare Dataset**\n",
        "   - Uses `kagglehub` to dynamically download images from the **Flickr8k dataset**\n",
        "\n",
        "3. **Extract Objects from Caption**\n",
        "   - Uses **spaCy** to extract nouns and proper nouns\n",
        "   - Expands object names using **WordNet synonyms**\n",
        "\n",
        "4. **Detect Objects in Image (YOLOv8)**\n",
        "   - Runs **YOLOv8** on the input image to detect objects\n",
        "   - Extracts bounding boxes and confidence scores\n",
        "\n",
        "5. **Match Detected Objects to Caption Objects (CLIP)**\n",
        "   - Computes text-image similarity between detected objects and expanded caption words\n",
        "   - Uses CLIP embeddings and cosine similarity to find best matches\n",
        "\n",
        "6. **Draw Results**\n",
        "   - Draws bounding boxes around matched objects\n",
        "   - Displays the final image with detected and matched objects\n",
        "\n",
        "## Notes:\n",
        "- **YOLOv8** provides fast, real-time object detection with bounding boxes.\n",
        "- **CLIP** enhances object matching by comparing image embeddings with text descriptions.\n",
        "- **WordNet** helps in expanding object names, improving text-object recognition.\n",
        "- The pipeline ensures better understanding of images based on **both detection and semantic matching**.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import spacy\n",
        "import kagglehub\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from collections import Counter\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Load models\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "model = YOLO(\"yolov8x.pt\")  # Large model for object detection\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Download dataset dynamically\n",
        "# path = kagglehub.dataset_download(\"adityajn105/flickr8k\")\n",
        "# dataset_path = os.path.join(path, \"Images\")\n",
        "\n",
        "def extract_objects(text):\n",
        "    \"\"\"Extracts important object names from a caption and handles synonyms using WordNet.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    words = [token.text.lower() for token in doc if token.pos_ in ['NOUN', 'PROPN'] and not token.is_stop]\n",
        "\n",
        "    # Get noun chunks (multi-word objects)\n",
        "    noun_chunks = [chunk.text.lower() for chunk in doc.noun_chunks]\n",
        "\n",
        "    # Use a Counter to track object occurrences\n",
        "    objects = Counter(words + noun_chunks)\n",
        "\n",
        "    # Expand words with synonyms to improve matching\n",
        "    expanded_objects = {}\n",
        "    for word in objects:\n",
        "        synonyms = set()\n",
        "        for synset in wordnet.synsets(word, pos=wordnet.NOUN):\n",
        "            for lemma in synset.lemmas():\n",
        "                synonyms.add(lemma.name().replace(\"_\", \" \"))\n",
        "        expanded_objects[word] = synonyms | {word}\n",
        "\n",
        "    return expanded_objects\n",
        "\n",
        "def detect_objects(image_path):\n",
        "    \"\"\"Detect objects in an image using YOLOv8.\"\"\"\n",
        "    results = model(image_path, conf=0.3)  # Adjusted threshold for better accuracy\n",
        "    return results\n",
        "\n",
        "def clip_similarity(image_path, detected_objects, text_objects):\n",
        "    \"\"\"Match detected objects with caption objects using CLIP embeddings.\"\"\"\n",
        "    matched_objects = []\n",
        "\n",
        "    # Convert detected objects into text labels\n",
        "    detected_labels = Counter([model.names[int(box.cls.item())] for result in detected_objects for box in result.boxes])\n",
        "\n",
        "    if not detected_labels:\n",
        "        print(\"No objects detected!\")\n",
        "        return []\n",
        "\n",
        "    # Load image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Prepare text queries: detected labels and expanded caption objects\n",
        "    caption_words = list({syn for syns in text_objects.values() for syn in syns})\n",
        "    queries = list(detected_labels.keys()) + caption_words\n",
        "\n",
        "    # CLIP processing\n",
        "    inputs = clip_processor(text=queries, images=image, return_tensors=\"pt\", padding=True)\n",
        "    outputs = clip_model(**inputs)\n",
        "\n",
        "    # Compute similarity scores\n",
        "    logits_per_image = outputs.logits_per_image\n",
        "    scores = logits_per_image.softmax(dim=1).detach().numpy()[0]\n",
        "\n",
        "    # Map text similarity back to objects\n",
        "    detected_count = len(detected_labels)\n",
        "    for i, label in enumerate(queries[:detected_count]):\n",
        "        highest_sim = max(scores[i + detected_count:])  # Compare detected objects to caption objects\n",
        "        if highest_sim > 0.15:  # Adjusted threshold\n",
        "            matched_objects.append(label)\n",
        "\n",
        "    return matched_objects\n",
        "\n",
        "def draw_boxes(image_path, results, matched_objects):\n",
        "    \"\"\"Draw bounding boxes around matched objects.\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(\"Error: Image not found.\")\n",
        "        return\n",
        "\n",
        "    for result in results:\n",
        "        for box in result.boxes:\n",
        "            obj_name = model.names[int(box.cls.item())]\n",
        "            if obj_name not in matched_objects:\n",
        "                continue  # Ignore unmatched objects\n",
        "\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy.tolist()[0])\n",
        "            prob = box.conf.item()\n",
        "\n",
        "            # Draw bounding box\n",
        "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(image, f\"{obj_name} ({prob:.2f})\", (x1, y1 - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "# Test with an image and caption\n",
        "# image_path = os.path.join(dataset_path, \"109738916_236dc456ac.jpg\")\n",
        "image_path = \"/content/20250326_232148.jpg\"\n",
        "caption = \"file , laptop , bottle , mouse , book , id , hearphone , document , page , table , chair , mat \"\n",
        "\n",
        "text_objects = extract_objects(caption)\n",
        "results = detect_objects(image_path)\n",
        "matched_objects = clip_similarity(image_path, results, text_objects)\n",
        "draw_boxes(image_path, results, matched_objects)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n7r8BsVEe-BA",
        "outputId": "fe0cfb39-e563-4786-a29a-c13c040aa13b"
      },
      "outputs": [],
      "source": [
        "#object detection in video using yolo nano\n",
        "\n",
        "\"\"\"\n",
        "This script performs **real-time object detection and optical flow tracking** on a video using **YOLOv8** and **Lucas-Kanade Optical Flow**.\n",
        "It processes video frames efficiently and overlays bounding boxes, labels, and motion tracking arrows.\n",
        "\n",
        "### Overall Process:\n",
        "1. **Initialize YOLOv8-Nano**: A lightweight object detection model that runs efficiently on CPU/GPU.\n",
        "2. **Load and process video**: Reads a video file and extracts its properties (FPS, resolution).\n",
        "3. **Frame skipping for efficiency**: Reduces FPS to `target_fps` (10 FPS) to speed up processing.\n",
        "4. **Run YOLOv8 on each frame**: Detects objects and extracts bounding boxes.\n",
        "5. **Apply Optical Flow Tracking**:\n",
        "   - Tracks motion of detected objects across frames using Lucas-Kanade Optical Flow.\n",
        "   - Draws motion arrows to indicate movement.\n",
        "6. **Draw Bounding Boxes & Labels**:\n",
        "   - Labels detected objects with their names and confidence scores.\n",
        "   - Uses alternating colors for better visualization.\n",
        "7. **Save and Display Processed Frames**:\n",
        "   - Saves output video with annotated detections.\n",
        "   - Displays each processed frame in **Google Colab**.\n",
        "   - Downloads the final processed video.\n",
        "\n",
        "### Libraries Used:\n",
        "- `cv2 (OpenCV)`: Handles video processing, drawing, and optical flow tracking.\n",
        "- `torch`: Enables hardware acceleration (CUDA support if available).\n",
        "- `ultralytics.YOLO`: Loads and runs YOLOv8 for object detection.\n",
        "- `numpy`: Handles array operations for bounding box and point tracking.\n",
        "- `google.colab.patches`: Displays frames in Google Colab.\n",
        "- `google.colab.files`: Downloads the processed video.\n",
        "\n",
        "### Performance Optimizations:\n",
        "- Uses **YOLOv8-Nano** (`yolov8n.pt`) for faster processing.\n",
        "- Implements **frame skipping** to reduce computational load.\n",
        "- Uses **Lucas-Kanade Optical Flow** for efficient object motion tracking.\n",
        "\n",
        "\"\"\"\n",
        "#u will get error at end bcz the display pipeline fo colab break\n",
        "#if cv2 used repeatedly for each frame and show the boxed\n",
        "\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from google.colab.patches import cv2_imshow  # For displaying images in Colab\n",
        "from google.colab import files\n",
        "\n",
        "# Load YOLOv8-Nano model (smallest version, fast on CPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "yolo_model = YOLO(\"yolov8n.pt\")  # No need to explicitly move to device\n",
        "\n",
        "# Load Video\n",
        "video_path = \"/content/1044-142621375_medium.mp4\"  # Change to your video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Get video properties\n",
        "original_fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# Target FPS\n",
        "target_fps = 10\n",
        "frame_skip = max(1, original_fps // target_fps)  # Ensure at least 1 frame is processed\n",
        "\n",
        "# Define video writer\n",
        "fourcc = cv2.VideoWriter_fourcc(*'avc1')  # MP4 format (H.264)\n",
        "output_file = \"output.mp4\"\n",
        "out = cv2.VideoWriter(output_file, fourcc, target_fps, (frame_width, frame_height))\n",
        "\n",
        "# Optical Flow Parameters\n",
        "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "prev_gray, prev_points = None, None\n",
        "frame_count = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    if frame_count % frame_skip != 0:\n",
        "        frame_count += 1\n",
        "        continue\n",
        "    frame_count += 1\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    results = yolo_model(frame)\n",
        "    detections = results[0].boxes.data.cpu().numpy()\n",
        "\n",
        "    if prev_gray is not None and prev_points is not None and len(prev_points) > 0:\n",
        "        new_points, status, _ = cv2.calcOpticalFlowPyrLK(prev_gray, gray, prev_points, None, **lk_params)\n",
        "        for i, (new, old) in enumerate(zip(new_points, prev_points)):\n",
        "            a, b = new.ravel()\n",
        "            c, d = old.ravel()\n",
        "            cv2.arrowedLine(frame, (int(c), int(d)), (int(a), int(b)), (255, 0, 0), 2)\n",
        "\n",
        "    prev_gray = gray.copy()\n",
        "    prev_points = np.array([[x1 + (x2 - x1) / 2, y1 + (y2 - y1) / 2] for (x1, y1, x2, y2, _, _) in detections], dtype=np.float32)\n",
        "\n",
        "    for (x1, y1, x2, y2, score, cls) in detections:\n",
        "        color = (0, 255, 255) if int(cls) % 2 == 0 else (255, 0, 255)  # Alternate colors\n",
        "        label = f\"{results[0].names[int(cls)]} ({score:.2f})\"\n",
        "\n",
        "        # Draw bounding box with thicker lines\n",
        "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 3)\n",
        "\n",
        "        # Draw label outside the box with an arrow\n",
        "        label_x, label_y = int(x1), max(int(y1) - 15, 10)\n",
        "        cv2.putText(frame, label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n",
        "        cv2.arrowedLine(frame, (label_x + 30, label_y), (int(x1), int(y1)), color, 2)\n",
        "\n",
        "    out.write(frame)\n",
        "    cv2_imshow(frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "files.download(output_file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}