{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install 'git+https://github.com/facebookresearch/detectron2.git'\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install numpy"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Detectron2 (Mask R-CNN with FPN and ResNet-50 Backbone)\n",
        "\n",
        "\"\"\"\n",
        "# Object Detection and Matching Pipeline using Detectron2 and CLIP\n",
        "\n",
        "## Overview:\n",
        "This script performs object detection on an image using Detectron2's **Mask R-CNN** model and matches detected objects to text descriptions using **CLIP (Contrastive Language-Image Pretraining)**. The goal is to find objects in an image that match the words extracted from a given caption.\n",
        "\n",
        "## Steps:\n",
        "1. **Load Dependencies**\n",
        "   - OpenCV (cv2) for image processing and visualization\n",
        "   - Detectron2 for object detection (Mask R-CNN with FPN and ResNet-50 backbone)\n",
        "   - CLIP from Hugging Face Transformers for text-image similarity\n",
        "   - spaCy for extracting object-related words from captions\n",
        "\n",
        "2. **Load Pre-trained Models**\n",
        "   - Detectron2 model (`mask_rcnn_R_50_FPN_3x`) for instance segmentation\n",
        "   - CLIP model (`openai/clip-vit-base-patch32`) for text-image similarity\n",
        "   - `spaCy` NLP model (`en_core_web_sm`) for extracting nouns from text\n",
        "\n",
        "3. **Process Image & Caption**\n",
        "   - Convert input caption into object-related keywords using spaCy\n",
        "   - Run Detectron2 model on the image to detect objects and extract bounding boxes\n",
        "   - Use CLIP to compute the similarity between detected objects and caption words\n",
        "\n",
        "4. **Draw Results**\n",
        "   - Draw bounding boxes around matched objects\n",
        "   - Display the final image with detected and matched objects\n",
        "\n",
        "## Notes:\n",
        "- **Detectron2** is used for object detection, which provides bounding boxes and object labels.\n",
        "- **CLIP** helps to match detected objects to textual descriptions based on image-text similarity.\n",
        "- The pipeline aims to identify objects from a caption and visually highlight them in an image.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import spacy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# Load NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load CLIP model for better text-image similarity\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Load Detectron2 model\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.MODEL.DEVICE = \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "\n",
        "def extract_objects(text):\n",
        "    \"\"\"Extract nouns (objects) from caption.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    objects = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
        "    return objects\n",
        "\n",
        "\n",
        "def detect_objects(image_path):\n",
        "    \"\"\"Use Detectron2 to detect objects in image.\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    outputs = predictor(image)\n",
        "    instances = outputs[\"instances\"]\n",
        "\n",
        "    detected_objects = []\n",
        "    boxes = instances.pred_boxes.tensor.cpu().numpy()\n",
        "    scores = instances.scores.cpu().numpy()\n",
        "    labels = instances.pred_classes.cpu().numpy()\n",
        "    metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
        "\n",
        "    for i, box in enumerate(boxes):\n",
        "        obj_name = metadata.thing_classes[labels[i]]\n",
        "        detected_objects.append((obj_name, box, scores[i]))\n",
        "\n",
        "    return detected_objects\n",
        "\n",
        "\n",
        "def clip_similarity(image_path, detected_objects, text_objects):\n",
        "    \"\"\"Use CLIP to match detected objects with text objects.\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    detected_labels = [obj[0] for obj in detected_objects]\n",
        "\n",
        "    # Process with CLIP\n",
        "    inputs = clip_processor(\n",
        "        text=detected_labels + text_objects,  # Flatten list\n",
        "        images=image,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    outputs = clip_model(**inputs)\n",
        "\n",
        "    # Extract similarity scores\n",
        "    text_features = outputs.text_embeds\n",
        "    image_features = outputs.image_embeds\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarity_matrix = torch.matmul(text_features, image_features.T).cpu().detach().numpy()\n",
        "\n",
        "    matched = []\n",
        "    for i, text_obj in enumerate(text_objects):\n",
        "        best_match_idx = np.argmax(similarity_matrix[i])\n",
        "        best_match_label = detected_labels[best_match_idx]\n",
        "        matched.append((best_match_label, detected_objects[best_match_idx][1], detected_objects[best_match_idx][2]))\n",
        "\n",
        "    return matched\n",
        "\n",
        "\n",
        "def draw_boxes(image_path, matches):\n",
        "    \"\"\"Draw bounding boxes around matched objects.\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    for obj_name, bbox, prob in matches:\n",
        "        x1, y1, x2, y2 = map(int, bbox)\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        cv2.putText(image, f\"{obj_name} ({prob:.2f})\", (x1, y1 - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# --- RUN PIPELINE ---\n",
        "image_path = \"/root/.cache/kagglehub/datasets/adityajn105/flickr8k/versions/1/Images/109738916_236dc456ac.jpg\"\n",
        "caption = \"person and motorcycles\"\n",
        "\n",
        "text_objects = extract_objects(caption)\n",
        "detected_objects = detect_objects(image_path)\n",
        "matched_objects = clip_similarity(image_path, detected_objects, text_objects)\n",
        "draw_boxes(image_path, matched_objects)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import spacy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# Load NLP model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load CLIP model for better text-image similarity\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Load Detectron2 config and model\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.3\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "cfg.MODEL.DEVICE = \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "\n",
        "def extract_objects(text):\n",
        "    \"\"\"Extract nouns (objects) from caption.\"\"\"\n",
        "    doc = nlp(text)\n",
        "    objects = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
        "    return objects\n",
        "\n",
        "\n",
        "def detect_objects(image_path):\n",
        "    \"\"\"Use Detectron2 to detect objects in image.\"\"\"\n",
        "    print(f\"[INFO] Checking image path: {image_path}\")\n",
        "    if not os.path.exists(image_path):\n",
        "        raise FileNotFoundError(f\"[ERROR] Image not found at path: {image_path}\")\n",
        "\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise ValueError(f\"[ERROR] Failed to load image at path: {image_path}\")\n",
        "\n",
        "    outputs = predictor(image)\n",
        "    instances = outputs[\"instances\"]\n",
        "\n",
        "    detected_objects = []\n",
        "    boxes = instances.pred_boxes.tensor.cpu().numpy()\n",
        "    scores = instances.scores.cpu().numpy()\n",
        "    labels = instances.pred_classes.cpu().numpy()\n",
        "\n",
        "    metadata = MetadataCatalog.get(cfg.DATASETS.TRAIN[0])\n",
        "\n",
        "    for i, box in enumerate(boxes):\n",
        "        obj_name = metadata.thing_classes[labels[i]]\n",
        "        detected_objects.append((obj_name, box, scores[i]))\n",
        "\n",
        "    return detected_objects\n",
        "\n",
        "\n",
        "def clip_similarity(image_path, detected_objects, text_objects):\n",
        "    \"\"\"Use CLIP to match detected objects with text objects.\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    detected_labels = [obj[0] for obj in detected_objects]\n",
        "\n",
        "    inputs = clip_processor(\n",
        "        text=detected_labels + text_objects,\n",
        "        images=image,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    outputs = clip_model(**inputs)\n",
        "\n",
        "    text_features = outputs.text_embeds\n",
        "    image_features = outputs.image_embeds\n",
        "\n",
        "    similarity_matrix = torch.matmul(text_features, image_features.T).cpu().detach().numpy()\n",
        "\n",
        "    matched = []\n",
        "    for i, text_obj in enumerate(text_objects):\n",
        "        best_match_idx = np.argmax(similarity_matrix[i])\n",
        "        best_match_label = detected_labels[best_match_idx]\n",
        "        matched.append((best_match_label, detected_objects[best_match_idx][1], detected_objects[best_match_idx][2]))\n",
        "\n",
        "    return matched\n",
        "\n",
        "\n",
        "def draw_boxes(image_path, matches):\n",
        "    \"\"\"Draw bounding boxes around matched objects.\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    for obj_name, bbox, prob in matches:\n",
        "        x1, y1, x2, y2 = map(int, bbox)\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        cv2.putText(image, f\"{obj_name} ({prob:.2f})\", (x1, y1 - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# --- RUN PIPELINE ---\n",
        "\n",
        "# ✅ Set a valid image path (verify this path exists and loads correctly)\n",
        "image_path = \"/content/20250326_232148.jpg\"\n",
        "caption = \"person and laptop , bottle ,bed , mat , chair , table\"\n",
        "\n",
        "# Run pipeline\n",
        "text_objects = extract_objects(caption)\n",
        "detected_objects = detect_objects(image_path)\n",
        "matched_objects = clip_similarity(image_path, detected_objects, text_objects)\n",
        "draw_boxes(image_path, matched_objects)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}