{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLF6G6aVoCEo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import inception_v3\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import pickle\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Path configuration\n",
        "base_path = 'archive (2)'\n",
        "image_folder = os.path.join(base_path, 'Images')  # Folder containing images\n",
        "captions_file = os.path.join(base_path, 'captions.txt')  # Captions text file\n",
        "\n",
        "# Load and preprocess captions\n",
        "def load_captions(filename):\n",
        "    \"\"\"Load captions and create mapping from image IDs to captions\"\"\"\n",
        "    captions_mapping = {}\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(',')\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            img_id, caption = parts[0], ','.join(parts[1:])\n",
        "            img_id = os.path.splitext(img_id)[0]  # Remove file extension\n",
        "            if img_id not in captions_mapping:\n",
        "                captions_mapping[img_id] = []\n",
        "            captions_mapping[img_id].append(caption.lower().strip())\n",
        "    return captions_mapping\n",
        "\n",
        "captions_mapping = load_captions(captions_file)\n",
        "\n",
        "# Initialize InceptionV3 for feature extraction\n",
        "def get_inception():\n",
        "    model = inception_v3(pretrained=True)\n",
        "    model.fc = nn.Identity()  # Remove final layer\n",
        "    model = model.to(device).eval()\n",
        "    return model\n",
        "\n",
        "image_model = get_inception()\n",
        "\n",
        "# Image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Feature extraction with progress tracking\n",
        "def extract_features(image_folder, captions_mapping):\n",
        "    features = {}\n",
        "    missing_images = []\n",
        "\n",
        "    for img_id in tqdm(captions_mapping.keys(), desc=\"Extracting features\"):\n",
        "        img_path = os.path.join(image_folder, f\"{img_id}.jpg\")\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                features[img_id] = image_model(img_tensor).cpu().numpy()\n",
        "        except Exception as e:\n",
        "            missing_images.append(img_id)\n",
        "            continue\n",
        "\n",
        "    print(f\"\\nFailed to process {len(missing_images)} images\")\n",
        "    return features\n",
        "\n",
        "features = extract_features(image_folder, captions_mapping)\n",
        "\n",
        "# Save features\n",
        "with open('image_features.pkl', 'wb') as f:\n",
        "    pickle.dump(features, f)\n",
        "\n",
        "# Vocabulary construction\n",
        "def build_vocabulary(captions_mapping):\n",
        "    word_counts = Counter()\n",
        "    for img_id in captions_mapping:\n",
        "        for caption in captions_mapping[img_id]:\n",
        "            word_counts.update(caption.split())\n",
        "\n",
        "    # Create vocabulary with special tokens\n",
        "    vocab = ['<pad>', '<start>', '<end>', '<unk>']\n",
        "    vocab += [word for word, count in word_counts.items() if count >= 5]\n",
        "\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "    return vocab, word2idx, idx2word\n",
        "\n",
        "vocab, word2idx, idx2word = build_vocabulary(captions_mapping)\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Find maximum caption length\n",
        "max_length = max(len(caption.split()) for captions in captions_mapping.values()\n",
        "                for caption in captions) + 2  # +2 for start/end tokens\n",
        "print(f\"Maximum caption length: {max_length}\")\n",
        "\n",
        "# Dataset class\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, img_ids, captions_mapping, features, word2idx, max_length):\n",
        "        self.img_ids = [img_id for img_id in img_ids if img_id in features]\n",
        "        self.captions_mapping = captions_mapping\n",
        "        self.features = features\n",
        "        self.word2idx = word2idx\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.img_ids[idx]\n",
        "        feature = torch.FloatTensor(self.features[img_id])\n",
        "        captions = self.captions_mapping[img_id]\n",
        "\n",
        "        # Randomly select one caption per image\n",
        "        caption = np.random.choice(captions)\n",
        "\n",
        "        # Convert caption to indices\n",
        "        caption_words = ['<start>'] + caption.split() + ['<end>']\n",
        "        caption_idx = [self.word2idx.get(word, self.word2idx['<unk>'])\n",
        "                      for word in caption_words]\n",
        "\n",
        "        # Pad to max_length\n",
        "        caption_idx = caption_idx[:self.max_length]\n",
        "        caption_idx += [self.word2idx['<pad>']] * (self.max_length - len(caption_idx))\n",
        "\n",
        "        return feature.squeeze(0), torch.LongTensor(caption_idx)\n",
        "\n",
        "# Model architecture with attention\n",
        "class ImageCaptionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=256, hidden_size=256):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.encoder = nn.Linear(2048, hidden_size)\n",
        "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        # Encode image features\n",
        "        img_embed = self.dropout(self.encoder(features))\n",
        "\n",
        "        # Embed captions\n",
        "        cap_embed = self.dropout(self.embed(captions))\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out, _ = self.lstm(cap_embed)\n",
        "\n",
        "        # Combine image and language features\n",
        "        img_embed = img_embed.unsqueeze(1).expand(-1, lstm_out.size(1), -1)\n",
        "        combined = img_embed + lstm_out\n",
        "\n",
        "        # Predict next words\n",
        "        output = self.decoder(combined)\n",
        "        return output\n",
        "\n",
        "# Initialize model\n",
        "model = ImageCaptionModel(vocab_size).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index\n",
        "\n",
        "# Split dataset\n",
        "img_ids = list(features.keys())\n",
        "split = int(0.8 * len(img_ids))\n",
        "train_ids = img_ids[:split]\n",
        "val_ids = img_ids[split:]\n",
        "\n",
        "train_dataset = ImageCaptionDataset(train_ids, captions_mapping, features, word2idx, max_length)\n",
        "val_dataset = ImageCaptionDataset(val_ids, captions_mapping, features, word2idx, max_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, train_loader, val_loader, epochs=20):\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        # Training phase\n",
        "        for features, captions in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "            features = features.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass - predict next words\n",
        "            outputs = model(features, captions[:, :-1])\n",
        "            loss = criterion(outputs.reshape(-1, vocab_size),\n",
        "                           captions[:, 1:].reshape(-1))\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for features, captions in val_loader:\n",
        "                features = features.to(device)\n",
        "                captions = captions.to(device)\n",
        "\n",
        "                outputs = model(features, captions[:, :-1])\n",
        "                loss = criterion(outputs.reshape(-1, vocab_size),\n",
        "                               captions[:, 1:].reshape(-1))\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        # Print statistics\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Start training\n",
        "trained_model = train_model(model, train_loader, val_loader)\n",
        "\n",
        "# Prediction function\n",
        "def generate_caption(model, image_path, max_length=20):\n",
        "    model.eval()\n",
        "\n",
        "    # Process image\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    # Extract features\n",
        "    with torch.no_grad():\n",
        "        features = image_model(img_tensor)\n",
        "\n",
        "    # Initialize caption\n",
        "    caption = ['<start>']\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        # Convert current caption to indices\n",
        "        caption_idx = [word2idx.get(word, word2idx['<unk>']) for word in caption]\n",
        "        caption_tensor = torch.LongTensor(caption_idx).unsqueeze(0).to(device)\n",
        "\n",
        "        # Predict next word\n",
        "        with torch.no_grad():\n",
        "            output = model(features, caption_tensor)\n",
        "            # Get the last predicted word (modification here)\n",
        "            next_word_idx = output[0, -1].argmax().item()\n",
        "            next_word = idx2word[next_word_idx]\n",
        "\n",
        "            caption.append(next_word)\n",
        "            if next_word == '<end>':\n",
        "                break\n",
        "\n",
        "    # Remove special tokens and join\n",
        "    caption = ' '.join([word for word in caption if word not in ['<start>', '<end>', '<pad>']])\n",
        "    return caption\n",
        "\n",
        "# Test on sample image\n",
        "# sample_image = os.path.join(image_folder, list(features.keys())[0] + '.jpg')\n",
        "# print(\"Generated caption:\", generate_caption(trained_model, sample_image))\n",
        "\n",
        "# test_image = os.path.join(base_path, 'Images', '10815824_2997e03d76.jpg')\n",
        "# if os.path.exists(test_image):\n",
        "#     caption = generate_caption(model, test_image)\n",
        "#     print(f\"Generated caption: {caption}\")\n",
        "# else:\n",
        "#     print(f\"Test image not found at path: {test_image}\")\n",
        "\n",
        "\n",
        "# List of test images (add more image filenames as needed)\n",
        "test_images = [\n",
        "    os.path.join(base_path, 'Images', '667626_18933d713e.jpg'),\n",
        "    os.path.join(base_path, 'Images', '3637013_c675de7705.jpg'),  # Replace with actual filenames\n",
        "    os.path.join(base_path, 'Images', '3639617775_149001232a.jpg'),   # Replace with actual filenames\n",
        "    os.path.join(base_path, 'Images', '3639547922_0b00fed5cd.jpg'),  # Replace with actual filenames\n",
        "    os.path.join(base_path, 'Images', '3639428663_dae5e8146e.jpg'),  # Replace with actual filenames\n",
        "    os.path.join(base_path, 'Images', '3639363462_bcdb21de29.jpg')  # Replace with actual filenames\n",
        "]\n",
        "\n",
        "# Iterate over each test image\n",
        "for test_image in test_images:\n",
        "    if os.path.exists(test_image):\n",
        "        # Generate caption\n",
        "        caption = generate_caption(model, test_image)\n",
        "\n",
        "        # Display the image and its caption\n",
        "        img = Image.open(test_image)\n",
        "        img.show()  # This works in Jupyter Notebook or IPython environments\n",
        "        print(f\"Generated caption: {caption}\")\n",
        "    else:\n",
        "        print(f\"Test image not found at path: {test_image}\")"
      ]
    }
  ]
}