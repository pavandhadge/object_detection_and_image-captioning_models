{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install torch  transformers pillow"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\"\"\"\n",
        "This script generates a caption for an image using the BLIP (Bootstrapped Language-Image Pretraining) model.\n",
        "It employs the Hugging Face `transformers` library and the `PIL` library for image handling.\n",
        "\n",
        "### Overall Process:\n",
        "1. Load the BLIP processor and model (`Salesforce/blip-image-captioning-base`), which is a pretrained vision-language model.\n",
        "2. Open and preprocess the image using PIL, ensuring it's in RGB format.\n",
        "3. Convert the image into tensors using the processor, making it compatible with the model.\n",
        "4. Pass the processed image to the BLIP model to generate a caption.\n",
        "5. Decode the generated caption and print the result.\n",
        "\n",
        "### Libraries Used:\n",
        "- `transformers`: Provides BLIP's processor and model for image captioning.\n",
        "- `PIL (Pillow)`: Handles image loading and processing.\n",
        "- `torch`: Supports tensor operations required for model input and output.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Load BLIP-2 Processor & Model\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "# Load Image\n",
        "image = Image.open(\"/content/20250326_232107.jpg\").convert(\"RGB\")\n",
        "\n",
        "# Preprocess Image\n",
        "inputs = processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "# Generate Caption\n",
        "caption_ids = model.generate(**inputs)\n",
        "caption = processor.decode(caption_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated Caption:\", caption)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#very large model --required more than 15 gb gpu for large model can use its small version if possible\n",
        "\"\"\"\n",
        "This script generates a highly detailed caption for an image using BLIP-2 (Bootstrapped Language-Image Pretraining 2).\n",
        "It utilizes the `transformers` library from Hugging Face and the `PIL` library for image handling.\n",
        "\n",
        "### Overall Process:\n",
        "1. Load the BLIP-2 processor and model (`Salesforce/blip2-opt-6.7b`), a vision-language model that integrates image and text processing.\n",
        "2. Open and preprocess the image using PIL, ensuring it's in RGB format.\n",
        "3. Convert the image into tensors and provide a text prompt to guide the caption generation.\n",
        "4. Use beam search (`num_beams=5`) and top-p sampling (`top_p=0.9`) to refine caption quality.\n",
        "5. Decode the generated output and print the final caption.\n",
        "\n",
        "### Libraries Used:\n",
        "- `transformers`: Provides BLIP-2's processor and model for advanced image captioning.\n",
        "- `PIL (Pillow)`: Handles image loading and processing.\n",
        "- `torch`: Supports tensor operations required for model input and output.\n",
        "\n",
        "### Notes:\n",
        "- The `text` parameter allows for prompt-based captioning, enabling more control over the generated description.\n",
        "- `max_length=100` ensures the caption is detailed.\n",
        "- `num_beams=5` improves caption quality using beam search.\n",
        "- `top_p=0.9` controls the randomness of predictions for better fluency.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Load BLIP-2 Processor & Model (Pretrained)\n",
        "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-6.7b\")  # Smaller models: \"Salesforce/blip2-opt-6.7b\"\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-6.7b\")\n",
        "\n",
        "# Load Image\n",
        "image = Image.open(\"/content/20250326_232107.jpg\").convert(\"RGB\")  # Change to your image path\n",
        "\n",
        "# Preprocess Image with a Descriptive Prompt\n",
        "inputs = processor(images=image, text=\"A highly detailed description of the image:\", return_tensors=\"pt\")\n",
        "\n",
        "# Generate Caption with Detailed Settings\n",
        "caption_ids = model.generate(**inputs, max_length=100, num_beams=5, top_p=0.9)\n",
        "caption = processor.tokenizer.decode(caption_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated Caption:\", caption)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# =========================\n",
        "# üîß INSTALL DEPENDENCIES\n",
        "# =========================\n",
        "!pip install -q transformers accelerate timm\n",
        "\n",
        "# =========================\n",
        "# üì¶ IMPORT LIBRARIES\n",
        "# =========================\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "\n",
        "# =========================\n",
        "# ‚öôÔ∏è SET DEVICE (T4 GPU FRIENDLY)\n",
        "# =========================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "# =========================\n",
        "# üß† LOAD BLIP-2 MODEL (FLAN-T5-XL + ViT-G/14)\n",
        "# =========================\n",
        "print(\"‚è≥ Loading BLIP-2 FLAN-T5-XL with ViT-G/14...\")\n",
        "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n",
        "model = Blip2ForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip2-flan-t5-xl\",\n",
        "    device_map=\"auto\",       # Spread model across available devices\n",
        "    torch_dtype=dtype        # Use float16 if on GPU\n",
        ")\n",
        "model.eval()\n",
        "print(\"‚úÖ Model loaded.\")\n",
        "\n",
        "# =========================\n",
        "# üñºÔ∏è UPLOAD IMAGE\n",
        "# =========================\n",
        "print(\"üì§ Upload an image...\")\n",
        "uploaded = files.upload()\n",
        "image_path = list(uploaded.keys())[0]\n",
        "image = Image.open(BytesIO(uploaded[image_path])).convert(\"RGB\")\n",
        "\n",
        "# =========================\n",
        "# ‚úèÔ∏è ASK FOR MAX TOKEN LENGTH\n",
        "# =========================\n",
        "try:\n",
        "    max_tokens = int(input(\"Enter max caption length (e.g., 20, 30, 50): \"))\n",
        "except:\n",
        "    max_tokens = 30\n",
        "    print(\"‚ö†Ô∏è Invalid input, defaulting to 30 tokens.\")\n",
        "\n",
        "# =========================\n",
        "# üìù GENERATE CAPTION\n",
        "# =========================\n",
        "print(\"üß† Generating caption...\")\n",
        "inputs = processor(images=image, return_tensors=\"pt\").to(device, dtype)\n",
        "generated_ids = model.generate(**inputs, max_new_tokens=max_tokens)\n",
        "\n",
        "caption = processor.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "print(\"üñãÔ∏è Caption:\", caption)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}