{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install torch transformers peft safetensors accelerate"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#not possible to run on colab\n",
        "\"\"\"\n",
        "This script generates a caption for an image using the **LLaVA (Large Language and Vision Assistant)** model.\n",
        "LLaVA is a vision-language model designed for image understanding and multimodal tasks.\n",
        "\n",
        "### Overall Process:\n",
        "1. **Load the LLaVA processor and model** (`llava-hf/llava-1.5-7b-hf`), a pretrained vision-language model.\n",
        "2. **Open and preprocess the image** using the PIL library.\n",
        "3. **Convert the image into tensors** using the processor to prepare it for model inference.\n",
        "4. **Pass the processed image to LLaVA** to generate a caption.\n",
        "5. **Decode and print the generated caption**.\n",
        "\n",
        "### Libraries Used:\n",
        "- `transformers`: Provides the LLaVA processor and model for image captioning.\n",
        "- `PIL (Pillow)`: Handles image loading and processing.\n",
        "- `torch`: Supports tensor operations required for model input and output.\n",
        "\n",
        "### Notes:\n",
        "- **LLaVA** is optimized for vision-language tasks, offering robust image captioning.\n",
        "- This script uses **LLaVA-1.5-7B**, a 7-billion parameter model, which balances performance and efficiency.\n",
        "- The model processes images without an explicit prompt, making it a general-purpose vision-language tool.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from transformers import LlavaProcessor, LlavaForConditionalGeneration\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Load LLaVA Processor & Model\n",
        "processor = LlavaProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
        "model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
        "\n",
        "# Load Image\n",
        "image = Image.open(\"/content/20250326_232107.jpg\")\n",
        "\n",
        "# Preprocess Image\n",
        "inputs = processor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "# Generate Caption\n",
        "caption_ids = model.generate(**inputs)\n",
        "caption = processor.decode(caption_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Generated Caption:\", caption)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}