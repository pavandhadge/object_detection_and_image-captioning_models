{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============== [CELL 1] ULTRA-OPTIMIZED SETUP ==============\n",
        "!pip install -q --no-cache-dir ultralytics torchvision Pillow matplotlib accelerate bitsandbytes transformers>=4.40\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image, ImageDraw, ImageOps,ImageFont\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration, BitsAndBytesConfig\n",
        "from matplotlib.patches import Rectangle\n",
        "import io"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Hardware optimization\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.set_float32_matmul_precision('high')\n",
        "print(\"setup complete\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "# ============== [CELL 2] TURBO IMAGE LOADING ==============\n",
        "def load_image(image_path, max_size=1024):\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        if hasattr(img, '_getexif'):\n",
        "            img = ImageOps.exif_transpose(img)\n",
        "        img = img.convert(\"RGB\")\n",
        "        if max(img.size) > max_size:\n",
        "            img.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
        "        print(f\"Image loaded .Size: {img.size}\")\n",
        "        return img\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading image: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============== [CELL 3] LIGHTNING OBJECT DETECTION ==============\n",
        "def init_detector(model_size='n'):\n",
        "    model = YOLO(f\"yolov8{model_size}.pt\", verbose=False)\n",
        "    model.fuse()  # Fuse layers for 30% speedup\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "    return model\n",
        "\n",
        "# Initialize once (cache this)\n",
        "detector = init_detector('n')  # 'nano' model - fastest\n",
        "\n",
        "# Turbo detection function\n",
        "def detect_objects(model, image_path):\n",
        "    results = model(image_path,\n",
        "                   imgsz=640,\n",
        "                   stream=True,  # Memory efficient\n",
        "                   half=True,    # FP16 inference\n",
        "                   device=0 if torch.cuda.is_available() else 'cpu')\n",
        "    return next(results)\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def draw_boxes(original_image, detections, conf_threshold=0.3):\n",
        "\n",
        "    try:\n",
        "        # ===== 1. INPUT VALIDATION =====\n",
        "        if not isinstance(original_image, Image.Image):\n",
        "            raise ValueError(\"Input must be PIL Image\")\n",
        "\n",
        "        orig_width, orig_height = original_image.size\n",
        "        print(f\"Original dimensions: {orig_width}x{orig_height}\")\n",
        "\n",
        "        # ===== 2. CREATE WORKING COPY =====\n",
        "        # Maintain original size for accurate coordinates\n",
        "        working_image = original_image.copy()\n",
        "        draw = ImageDraw.Draw(working_image)\n",
        "\n",
        "        # ===== 3. COORDINATE VERIFICATION =====\n",
        "        boxes = detections.boxes\n",
        "        if len(boxes) == 0:\n",
        "            print(\"‚ö†Ô∏è No detections to draw!\")\n",
        "            return working_image\n",
        "\n",
        "        # Convert to numpy once\n",
        "        boxes_np = boxes.xyxy.cpu().numpy()\n",
        "        print(f\"First box coordinates (raw): {boxes_np[0]}\")\n",
        "\n",
        "        # ===== 4. SCALE CHECK =====\n",
        "        # Verify if boxes match image dimensions\n",
        "        box_scale_x = orig_width / boxes_np[0][2] if boxes_np[0][2] > orig_width else 1.0\n",
        "        box_scale_y = orig_height / boxes_np[0][3] if boxes_np[0][3] > orig_height else 1.0\n",
        "\n",
        "        if box_scale_x != 1.0 or box_scale_y != 1.0:\n",
        "            print(f\"‚ö†Ô∏è Scaling required: X={box_scale_x:.2f}, Y={box_scale_y:.2f}\")\n",
        "            boxes_np[:, [0,2]] *= box_scale_x  # Scale x coordinates\n",
        "            boxes_np[:, [1,3]] *= box_scale_y  # Scale y coordinates\n",
        "            print(f\"Scaled first box: {boxes_np[0]}\")\n",
        "        else:\n",
        "            print(\"‚úÖ Coordinates match image dimensions\")\n",
        "\n",
        "        # ===== 5. DRAWING PARAMETERS =====\n",
        "        # Dynamic sizing based on original dimensions\n",
        "        font_size = max(12, int(orig_height / 50))\n",
        "        box_width = max(2, int(orig_height / 300))\n",
        "\n",
        "        try:\n",
        "            font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
        "        except:\n",
        "            font = ImageFont.load_default()\n",
        "\n",
        "        # ===== 6. DRAWING LOOP =====\n",
        "        for box, conf, cls in zip(boxes_np, boxes.conf.cpu().numpy(),\n",
        "                                 boxes.cls.cpu().numpy().astype(int)):\n",
        "            if conf < conf_threshold:\n",
        "                continue\n",
        "\n",
        "            # Convert coordinates to integers\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "\n",
        "            # Draw bounding box\n",
        "            draw.rectangle([x1, y1, x2, y2],\n",
        "                          outline=(255,0,0),\n",
        "                          width=box_width)\n",
        "\n",
        "            # Prepare label\n",
        "            label = f\"{detections.names[cls]} {conf:.2f}\"\n",
        "            text_bbox = draw.textbbox((0, 0), label, font=font)\n",
        "\n",
        "            # Label positioning with boundary checks\n",
        "            label_x = min(max(x1, 5), orig_width - (text_bbox[2]-text_bbox[0]) - 5)\n",
        "            label_y = max(y1 - (text_bbox[3]-text_bbox[1]) - 5, 5)\n",
        "\n",
        "            # Draw label background\n",
        "            draw.rectangle(\n",
        "                [label_x-2, label_y-2,\n",
        "                 label_x + (text_bbox[2]-text_bbox[0]) + 2,\n",
        "                 label_y + (text_bbox[3]-text_bbox[1]) + 2],\n",
        "                fill=(0,0,0)\n",
        "            )\n",
        "\n",
        "            # Draw text\n",
        "            draw.text((label_x, label_y), label, fill=(255,255,255), font=font)\n",
        "\n",
        "        print(\"‚úÖ Drawing completed successfully\")\n",
        "        return working_image\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"üö® Critical error: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return original_image  # Return original if drawing fails"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============== [CELL 5] HYPER-PROMPT ENGINEERING ==============\n",
        "def generate_prompt(results):\n",
        "    if not hasattr(results, 'boxes') or len(results.boxes) == 0:\n",
        "        return \"Describe this image in detail.\"\n",
        "\n",
        "    boxes = results.boxes.cpu()\n",
        "    high_conf = boxes.conf > 0.35\n",
        "    names = [results.names[int(cls)] for cls in boxes.cls[high_conf]]\n",
        "    unique_objs = np.unique(names)\n",
        "    print(unique_objs)\n",
        "\n",
        "    if len(unique_objs) == 1:\n",
        "        return f\"Detail the {unique_objs[0]}'s appearance and surroundings concisely.\"\n",
        "    elif len(unique_objs) == 2:\n",
        "        return f\"Describe the relationship between the {unique_objs[0]} and {unique_objs[1]} in this image.\"\n",
        "    elif len(unique_objs) > 2:\n",
        "        objects_str = \", \".join(unique_objs[:-1]) + f\", and {unique_objs[-1]}\"\n",
        "        return f\"Describe this image containing {objects_str}. Include appearance, position, and interaction.\"\n",
        "    return f\"Summarize this scene with {len(unique_objs)} objects.\"\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============== [CELL 6] BLIP-ROCKET CAPTIONING ==============\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "processor = InstructBlipProcessor.from_pretrained(\n",
        "    \"Salesforce/instructblip-flan-t5-xl\",\n",
        "    truncation_side=\"left\"  # Important for instruction prompts\n",
        ")\n",
        "model = InstructBlipForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/instructblip-flan-t5-xl\",\n",
        "    quantization_config=quant_config,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    # attn_implementation=\"flash_attention_2\"  # If available and compatible\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@torch.inference_mode()\n",
        "def generate_caption(image, prompt):\n",
        "    try:\n",
        "        inputs = processor(images=image, text=prompt, return_tensors=\"pt\", truncation=True , padding=True,)\n",
        "        inputs = inputs.to(model.device)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            num_beams=3,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,          # Add top-p sampling\n",
        "            repetition_penalty=1.25,  # Reduce repetition\n",
        "            early_stopping=True,\n",
        "            do_sample=True,      # Enable sampling for more diverse outputs\n",
        "            use_cache=True       # Enable KV cache for faster generation\n",
        "        )\n",
        "        return processor.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Caption generation failed: {str(e)}\")\n",
        "        return \"Could not generate description.\"\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "image_path = \"/content/images/000000000139.jpg\"  # <--- CHANGE THIS\n",
        "image = load_image(image_path)\n",
        "\n",
        "if image:\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "detections = detect_objects(detector, image_path)\n",
        "# print(detections)\n",
        "# print(f\"Detected {len(detections.boxes)} objects \")\n",
        "\n",
        "# After your detection code\n",
        "result_image = draw_boxes(image, detections)\n",
        "\n",
        "if result_image:\n",
        "    # Display without whitespace (for notebooks)\n",
        "    plt.figure(figsize=(result_image.width/100, result_image.height/100), dpi=100)\n",
        "    plt.imshow(result_image)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout(pad=0)  # Critical for removing whitespace\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Failed to generate result image\")\n",
        "\n",
        "prompt = generate_prompt(detections)\n",
        "print(f\"Optimized Prompt: {prompt}\")\n",
        "\n",
        "caption = generate_caption(image, prompt)\n",
        "print(\"\\nTurbo Caption:\")\n",
        "print(\"=\" * 60)\n",
        "print(caption)\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install fpdf2  # For PDF generation"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from fpdf import FPDF\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Directory setup\n",
        "image_dir = \"/content/images/test\"  # <-- Your input directory\n",
        "output_dir = \"/content/results\"  # <-- Output directory\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "class PDFReport(FPDF):\n",
        "    def header(self):\n",
        "        self.set_font('Arial', 'B', 16)\n",
        "        self.cell(0, 10, 'Image Analysis Comprehensive Report', 0, 1, 'C')\n",
        "        self.ln(10)\n",
        "\n",
        "    def add_image_section(self, title, image_path, max_width=160, max_height=120):\n",
        "        \"\"\"\n",
        "        Adds an image section with consistent sizing and proper layout\n",
        "        - Automatically handles portrait/landscape orientation\n",
        "        - Maintains aspect ratio\n",
        "        - Centers images horizontally\n",
        "        - Ensures consistent spacing\n",
        "        \"\"\"\n",
        "        self.set_font('Arial', 'B', 12)\n",
        "        self.cell(0, 8, title, 0, 1)  # Reduced title spacing\n",
        "\n",
        "        # Get image dimensions while maintaining aspect ratio\n",
        "        with Image.open(image_path) as img:\n",
        "            img_width, img_height = img.size\n",
        "            aspect_ratio = img_width / img_height\n",
        "\n",
        "            # Calculate dimensions to fit within our max bounds\n",
        "            if aspect_ratio > 1:  # Landscape\n",
        "                width = min(max_width, img_width)\n",
        "                height = width / aspect_ratio\n",
        "            else:  # Portrait\n",
        "                height = min(max_height, img_height)\n",
        "                width = height * aspect_ratio\n",
        "\n",
        "            # Center horizontally (A4 page is 210mm wide)\n",
        "            x_pos = (210 - width) / 2\n",
        "\n",
        "            # Add image with calculated dimensions\n",
        "            self.image(image_path,\n",
        "                    x=x_pos,\n",
        "                    w=width,\n",
        "                    h=height,\n",
        "                    keep_aspect_ratio=True)\n",
        "\n",
        "        # Consistent vertical spacing\n",
        "        self.ln(8 if height < 80 else 12)  # More space after tall images\n",
        "\n",
        "        # Add subtle divider line (light gray)\n",
        "        self.set_draw_color(200, 200, 200)\n",
        "        self.line(10, self.get_y(), 200, self.get_y())\n",
        "        self.ln(5)\n",
        "\n",
        "    def add_text_section(self, title, body):\n",
        "        self.set_font('Arial', 'B', 12)\n",
        "        self.cell(0, 10, title, 0, 1)\n",
        "        self.set_font('Arial', '', 10)\n",
        "        self.multi_cell(0, 5, body)\n",
        "        self.ln(8)\n",
        "\n",
        "    def footer(self):\n",
        "        self.set_y(-15)\n",
        "        self.set_font('Arial', 'I', 8)\n",
        "        self.cell(0, 10, f'Page {self.page_no()}', 0, 0, 'C')\n",
        "\n",
        "# Initialize single PDF report\n",
        "pdf = PDFReport()\n",
        "pdf.set_auto_page_break(auto=True, margin=15)\n",
        "pdf.add_page()\n",
        "\n",
        "# Process each image\n",
        "for idx, filename in enumerate(sorted(os.listdir(image_dir))):\n",
        "    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "        image_path = os.path.join(image_dir, filename)\n",
        "        print(f\"\\n{'='*60}\\nProcessing: {filename}\\n{'='*60}\")\n",
        "\n",
        "        # Add section separator for new image\n",
        "        if idx > 0:\n",
        "            pdf.add_page()\n",
        "            pdf.set_font('Arial', 'B', 14)\n",
        "            pdf.cell(0, 10, f\"Image Analysis: {filename}\", 0, 1)\n",
        "            pdf.ln(5)\n",
        "\n",
        "        # Load image\n",
        "        image = load_image(image_path)\n",
        "        if not image:\n",
        "            continue\n",
        "\n",
        "        # Save original image temporarily\n",
        "        original_path = os.path.join(output_dir, f\"temp_original_{idx}.jpg\")\n",
        "        image.save(original_path)\n",
        "\n",
        "        # Add original image to report\n",
        "        pdf.add_image_section(\"Original Image:\", original_path)\n",
        "\n",
        "        # Run detection\n",
        "        detections = detect_objects(detector, image_path)\n",
        "        print(f\"‚ö° Detected {len(detections.boxes)} objects\")\n",
        "\n",
        "        # Generate detection list\n",
        "        detected_objects = \"\\n\".join(\n",
        "            [f\"- {detections.names[int(cls)]}: {conf:.2f} confidence\"\n",
        "             for cls, conf in zip(detections.boxes.cls.cpu().numpy(),\n",
        "                                detections.boxes.conf.cpu().numpy())]\n",
        "        )\n",
        "        pdf.add_text_section(\n",
        "            f\"Detected Objects ({len(detections.boxes)}):\",\n",
        "            detected_objects\n",
        "        )\n",
        "\n",
        "        # Generate and add annotated image\n",
        "        annotated_img = draw_boxes(image, detections)\n",
        "        if annotated_img:\n",
        "            annotated_path = os.path.join(output_dir, f\"temp_annotated_{idx}.jpg\")\n",
        "            annotated_img.save(annotated_path)\n",
        "            pdf.add_image_section(\"Annotated Image:\", annotated_path)\n",
        "\n",
        "        # Generate and add prompt\n",
        "        prompt = generate_prompt(detections)\n",
        "        pdf.add_text_section(\"Generated Prompt:\", prompt)\n",
        "\n",
        "        # Generate and add caption\n",
        "        caption = generate_caption(image, prompt)\n",
        "        pdf.add_text_section(\"Generated Caption:\", caption)\n",
        "\n",
        "        # Clean up temp files\n",
        "        os.remove(original_path)\n",
        "        if annotated_img:\n",
        "            os.remove(annotated_path)\n",
        "\n",
        "# Save final report\n",
        "report_path = os.path.join(output_dir, \"comprehensive_report.pdf\")\n",
        "pdf.output(report_path)\n",
        "print(f\"\\n{'='*60}\\n‚úÖ Saved comprehensive report to: {report_path}\\n{'='*60}\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}