{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install --upgrade --force-reinstall torch torchvision torchaudio"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import zipfile\n",
        "import gdown\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import inception_v3, Inception_V3_Weights\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import pickle"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ==================== Step 1: Download dataset ====================\n",
        "base_path = 'archive'\n",
        "os.makedirs(base_path, exist_ok=True)\n",
        "dataset_id = ''\n",
        "captions_id = ''\n",
        "\n",
        "dataset_zip_path = os.path.join(base_path, 'flickr_dataset.zip')\n",
        "if not os.path.exists(dataset_zip_path):\n",
        "    print(\"Downloading dataset...\")\n",
        "    gdown.download(id=dataset_id, output=dataset_zip_path, quiet=False)\n",
        "\n",
        "if not os.path.exists(os.path.join(base_path, 'Images')):\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(base_path)\n",
        "\n",
        "captions_file = os.path.join(base_path, 'captions.txt')\n",
        "if not os.path.exists(captions_file):\n",
        "    print(\"Downloading captions.txt...\")\n",
        "    gdown.download(id=captions_id, output=captions_file, quiet=False)\n",
        "\n",
        "image_folder = os.path.join(base_path, 'Images')"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ==================== Step 2: Load captions ====================\n",
        "def load_captions(filename):\n",
        "    captions_mapping = {}\n",
        "    with open(filename, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(',')\n",
        "            if len(parts) < 2:\n",
        "                continue\n",
        "            img_id, caption = parts[0], ','.join(parts[1:])\n",
        "            img_id = os.path.splitext(img_id)[0]\n",
        "            if img_id not in captions_mapping:\n",
        "                captions_mapping[img_id] = []\n",
        "            captions_mapping[img_id].append(caption.lower().strip())\n",
        "    return captions_mapping\n",
        "\n",
        "captions_mapping = load_captions(captions_file)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ==================== Step 3: Feature Extraction ====================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "def get_inception():\n",
        "    weights = Inception_V3_Weights.DEFAULT\n",
        "    model = inception_v3(weights=weights, aux_logits=True)  # MUST be True when loading pretrained weights\n",
        "    model.aux_logits = False  # Disable aux output after loading\n",
        "    model.fc = nn.Identity()  # Remove classification layer\n",
        "    model = model.to(device).eval()\n",
        "    return model\n",
        "\n",
        "image_model = get_inception()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "def extract_features(image_folder, captions_mapping):\n",
        "    features = {}\n",
        "    for img_id in tqdm(captions_mapping.keys(), desc=\"Extracting features\"):\n",
        "        img_path = os.path.join(image_folder, f\"{img_id}.jpg\")\n",
        "        try:\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                features[img_id] = image_model(img_tensor).cpu().numpy()\n",
        "        except:\n",
        "            continue\n",
        "    return features\n",
        "\n",
        "features = extract_features(image_folder, captions_mapping)\n",
        "\n",
        "with open('image_features.pkl', 'wb') as f:\n",
        "    pickle.dump(features, f)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ==================== Step 4: Vocabulary ====================\n",
        "def build_vocabulary(captions_mapping):\n",
        "    word_counts = Counter()\n",
        "    for img_id in captions_mapping:\n",
        "        for caption in captions_mapping[img_id]:\n",
        "            word_counts.update(caption.split())\n",
        "\n",
        "    vocab = ['<pad>', '<start>', '<end>', '<unk>']\n",
        "    vocab += [word for word, count in word_counts.items() if count >= 5]\n",
        "\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "    return vocab, word2idx, idx2word\n",
        "\n",
        "vocab, word2idx, idx2word = build_vocabulary(captions_mapping)\n",
        "vocab_size = len(vocab)\n",
        "max_length = max(len(caption.split()) for captions in captions_mapping.values()\n",
        "                 for caption in captions) + 2"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ==================== Step 5: Dataset ====================\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, img_ids, captions_mapping, features, word2idx, max_length):\n",
        "        self.img_ids = [img_id for img_id in img_ids if img_id in features]\n",
        "        self.captions_mapping = captions_mapping\n",
        "        self.features = features\n",
        "        self.word2idx = word2idx\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_id = self.img_ids[idx]\n",
        "        feature = torch.FloatTensor(self.features[img_id])\n",
        "        captions = self.captions_mapping[img_id]\n",
        "        caption = np.random.choice(captions)\n",
        "\n",
        "        caption_words = ['<start>'] + caption.split() + ['<end>']\n",
        "        caption_idx = [self.word2idx.get(word, self.word2idx['<unk>'])\n",
        "                       for word in caption_words]\n",
        "        caption_idx = caption_idx[:self.max_length]\n",
        "        caption_idx += [self.word2idx['<pad>']] * (self.max_length - len(caption_idx))\n",
        "        return feature.squeeze(0), torch.LongTensor(caption_idx)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ==================== Step 6: Model ====================\n",
        "class ImageCaptionModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=256, hidden_size=256):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.encoder = nn.Linear(2048, hidden_size)\n",
        "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        img_embed = self.dropout(self.encoder(features))\n",
        "        cap_embed = self.dropout(self.embed(captions))\n",
        "        lstm_out, _ = self.lstm(cap_embed)\n",
        "        img_embed = img_embed.unsqueeze(1).expand(-1, lstm_out.size(1), -1)\n",
        "        combined = img_embed + lstm_out\n",
        "        output = self.decoder(combined)\n",
        "        return output\n",
        "\n",
        "model = ImageCaptionModel(vocab_size).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ==================== Step 7: Train Model ====================\n",
        "img_ids = list(features.keys())\n",
        "split = int(0.8 * len(img_ids))\n",
        "train_ids, val_ids = img_ids[:split], img_ids[split:]\n",
        "\n",
        "train_dataset = ImageCaptionDataset(train_ids, captions_mapping, features, word2idx, max_length)\n",
        "val_dataset = ImageCaptionDataset(val_ids, captions_mapping, features, word2idx, max_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=20):\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for features, captions in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "            features, captions = features.to(device), captions.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(features, captions[:, :-1])\n",
        "            loss = criterion(outputs.reshape(-1, vocab_size), captions[:, 1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for features, captions in val_loader:\n",
        "                features, captions = features.to(device), captions.to(device)\n",
        "                outputs = model(features, captions[:, :-1])\n",
        "                loss = criterion(outputs.reshape(-1, vocab_size), captions[:, 1:].reshape(-1))\n",
        "                val_loss += loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "    return model\n",
        "\n",
        "trained_model = train_model(model, train_loader, val_loader)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ==================== Step 8: Caption Generation ====================\n",
        "def generate_caption(model, image_path, max_length=20):\n",
        "    model.eval()\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        features = image_model(img_tensor)\n",
        "    caption = ['<start>']\n",
        "    for _ in range(max_length):\n",
        "        caption_idx = [word2idx.get(word, word2idx['<unk>']) for word in caption]\n",
        "        caption_tensor = torch.LongTensor(caption_idx).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(features, caption_tensor)\n",
        "            next_word_idx = output[0, -1].argmax().item()\n",
        "            next_word = idx2word[next_word_idx]\n",
        "            caption.append(next_word)\n",
        "            if next_word == '<end>':\n",
        "                break\n",
        "    caption = ' '.join([word for word in caption if word not in ['<start>', '<end>', '<pad>']])\n",
        "    return caption"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ==================== Step 9: Test ====================\n",
        "sample_image = os.path.join(image_folder, list(features.keys())[1] + '.jpg')\n",
        "if os.path.exists(sample_image):\n",
        "    caption = generate_caption(trained_model, sample_image)\n",
        "    print(\"Generated caption:\", caption)\n",
        "else:\n",
        "    print(\"Sample image not found.\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 0
}